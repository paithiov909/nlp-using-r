---
title: "NLP100knocks：形態素解析"
author: "paithiov909"
date: "`r Sys.Date()`"
output: html_document
---

# NLP100knocks：形態素解析

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
  #, fig.keep = "none" ## Run all chunks but never save new figures.
)
require(magrittr)
```

## データの読み込み

テキストを読みこむだけなら本当は何でもよいのですが、ここでは次のような形のデータフレームにして持ちます。

```{r morph-tif}
neko <- (function(){
  text <- readr::read_lines("https://nlp100.github.io/data/neko.txt", skip_empty_rows = TRUE)
  return(data.frame(doc_id = seq_along(text), text = text, stringsAsFactors = FALSE))
})()

str(neko)
```

この形のデータフレームは、[Text Interchange Formats（TIF）](https://docs.ropensci.org/tif/)という仕様を念頭においたものです。

TIFに準拠した強力なRパッケージとして、[quanteda](https://quanteda.io/)があります。quantedaはTIFに準拠した独自のS4クラス（`corpus`, `tokens`, `dfm`など）を実装していて、とくに文書単語行列（Document-Term Matrix, DTM. quantedaではDocument-Feature Matrix, DFMと呼ばれている）を同じ形のデータフレームを介さずに疎行列オブジェクトとして持つことができるため、比較的大きめのテキストを扱ってもメモリ効率がよいという利点があります。

## 形態素解析

### 30. 形態素解析結果の読み込み

あらかじめ上の形のデータフレームとして文書集合を持っておくと、次のようにquantedaのcorpusオブジェクトに変換できます。

```{r morph-corpus}
temp <- quanteda::corpus(neko)
```

ただ、100本ノックではtokenごとに品詞情報を確認する処理が多く、quantedaを使うメリットはあまりないため、[RcppMeCab](https://github.com/junhewk/RcppMeCab)の返すデータフレームをそのまま使っていきます。

GitHubにある開発版でも動くのですが、やや処理が遅いため、以下では独自にリファクタリングしたもの（[paithiov909/RcppMeCab](https://github.com/paithiov909/RcppMeCab)）を使っています。なお、現在CRANにある最新のRcppMeCab（v.0.0.1.2）はWindows環境だとインストールにコケるはずなので、CRANにあるものを使う場合にはUNIX系の環境が必要です。

なお、このforkにあるRcppMeCabでは、特にオプションを指定しないかぎり、内部的に機械的に文区切りされます（ICUの[Boundary Analysis](https://unicode-org.github.io/icu/userguide/boundaryanalysis/)の仕様については、[UAX#29](https://www.unicode.org/reports/tr29/)を参照のこと）。

```{r morph-30}
neko_txt_mecab <- neko %>%
  dplyr::pull("text") %>%
  RcppMeCab::posParallel(format = "data.frame")

str(neko_txt_mecab)
```

### 31. 動詞

```{r morph-31}
neko_txt_mecab %>%
  dplyr::filter(pos == "動詞") %>%
  dplyr::select(token) %>%
  head()
```

### 32. 動詞の原形

省略。RcppMeCabでは表層形（surface form）しか取れません。

### 33. 「AのB」

```{r morph-33}
neko_txt_mecab %>%
  tibble::rowid_to_column() %>%
  dplyr::filter(token == "の") %>%
  dplyr::pull(rowid) %>%
  purrr::keep(~ neko_txt_mecab$pos[. - 1] == "名詞" && neko_txt_mecab$pos[. + 1] == "名詞") %>%
  purrr::map_chr(~ stringr::str_c(
    neko_txt_mecab$token[. - 1],
    neko_txt_mecab$token[.],
    neko_txt_mecab$token[. + 1],
    collapse = ""
  )) %>%
  head(30L)
```

### 34. 名詞の連接

これよくわからない（もっと「Rらしい」書き方があるような気がする）。

Rのlistやvector（データフレームの「列」を含む）は、基本的に再代入するたびにメモリコピーが走るため、ループの内部などでサイズの大きいオブジェクトの変更を繰り返すと、非常に時間がかかってしまいます。やむをえずこのような処理をしたい場合、要素の削除については、削除したい要素を[zap](https://rlang.r-lib.org/reference/zap.html)というオブジェクトで置き換えるような書き方をすると、比較的現実的な時間内で処理できます。

```{r morph-34}
idx <- neko_txt_mecab %>%
  tibble::rowid_to_column() %>%
  dplyr::filter(pos == "名詞") %>%
  dplyr::pull(rowid) %>%
  purrr::discard(~ neko_txt_mecab$pos[. + 1] != "名詞")

search_in <- as.vector(idx, mode = "list") # as.listより速い（たぶん）

purrr::map_chr(search_in, function(idx) {
  itr <- idx
  res <- neko_txt_mecab$token[idx]
  while (neko_txt_mecab$pos[itr + 1] == "名詞") {
    res <- stringr::str_c(res, neko_txt_mecab$token[itr + 1])
    search_in <<- purrr::list_modify(
      search_in,
      !!!purrr::set_names(list(rlang::zap()), itr + 1))
    itr <- itr + 1
    next
  }
  return(res)
}) %>%
  head(30L)
```

### 35. 単語の出現頻度

```{r morph-35-a}
neko_txt_mecab %>%
  dplyr::group_by(token) %>%
  dplyr::count(token, sort = TRUE) %>%
  dplyr::ungroup() %>%
  head()
```

これだと助詞ばかりでつまらないので、ストップワードを除外してみます。

```{r moprh-35-b}
stopwords <-
  rtweet::stopwordslangs %>%
  dplyr::filter(lang == "ja") %>%
  dplyr::filter(p >= .98) %>%
  dplyr::pull(word)

`%without%` <- Negate(`%in%`)

neko_txt_mecab %>%
  dplyr::filter(pos != "記号") %>%
  dplyr::filter(token %without% stopwords) %>%
  dplyr::group_by(token) %>%
  dplyr::count(token, sort = TRUE) %>%
  dplyr::ungroup() %>%
  head()
```

### 36. 頻度上位10語

```{r morph-36}
neko_txt_mecab %>%
  dplyr::filter(pos != "記号") %>%
  dplyr::filter(token %without% stopwords) %>%
  dplyr::group_by(token) %>%
  dplyr::count(token, sort = TRUE) %>%
  dplyr::ungroup() %>%
  head(10L) %>%
  ggplot2::ggplot(aes(x = reorder(token, -n), y = n)) +
  ggplot2::geom_col() +
  ggplot2::labs(x = "token") +
  ggplot2::theme_light()
```

### 37. 「猫」と共起頻度の高い上位10語

解釈のしかたが複数あるけれど、ここでは段落ごとのbi-gramを数えることにします。

```{r morph-37}
neko_txt_mecab %>%
  tibble::rowid_to_column() %>%
  dplyr::filter(token == "猫") %>%
  dplyr::mutate(Collocation = stringr::str_c(token, neko_txt_mecab$token[rowid + 1], sep = " - ")) %>%
  dplyr::group_by(doc_id, Collocation) %>%
  dplyr::count(Collocation, sort = TRUE) %>%
  dplyr::ungroup() %>%
  head(10L) %>%
  ggplot2::ggplot(aes(x = reorder(Collocation, -n), y = n)) +
  ggplot2::geom_col() +
  ggplot2::labs(x = "Collocation", y = "Freq") +
  ggplot2::theme_light()
```

### 38. ヒストグラム

```{r morph-38}
neko_txt_mecab %>%
  dplyr::group_by(token) %>%
  dplyr::count(token) %>%
  ggplot2::ggplot(aes(x = reorder(token, -n), y = n)) +
  ggplot2::geom_col() +
  ggplot2::labs(x = NULL, y = "Freq") +
  ggplot2::scale_x_discrete(breaks = NULL) +
  ggplot2::scale_y_log10() +
  ggplot2::theme_light()
```

### 39. Zipfの法則

```{r morph-39}
count <- neko_txt_mecab %>%
  dplyr::group_by(token) %>%
  dplyr::count(token) %>%
  dplyr::ungroup()
count %>%
  tibble::rowid_to_column() %>%
  dplyr::mutate(rank = nrow(count) + 1 - dplyr::min_rank(count$n)[rowid]) %>%
  ggplot2::ggplot(aes(x = rank, y = n)) +
  ggplot2::geom_point() +
  ggplot2::labs(x = "Rank of Freq", y = "Freq") +
  ggplot2::scale_x_log10() +
  ggplot2::scale_y_log10() +
  ggplot2::theme_light()
```

## セッション情報

```{r sessioninfo}
sessioninfo::session_info()
```


