---
title: "NLP100knocks：準備運動・UNIXコマンド・正規表現"
author: "paithiov909"
date: "`r Sys.Date()`"
output: html_document
---

# NLP100knocks：準備運動・UNIXコマンド・正規表現

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
  #, fig.keep = "none" ## Run all chunks but never save new figures.
)
require(magrittr)
```

## 準備運動

コーディングの方針として、文字はなるべくリストのまま持っておいて最後に`unlist`する感じにしています。

よく知られているように、Rにおける文字列は、実体としては、文字列ベクトル（character vector）です。つまり、たとえば`str <- "aaa"`などとしても、このstrは長さが1のベクトルになります。
このため、R言語を触っている人たちのあいだでは、文字列ベクトルの一要素としてのいわゆる「文字列」のことは、string scalarとか単にstringなどと呼ばれます（たぶん）。

こうした事情から、R言語レベルで「文字列（string scalar）」の文字ごとに処理を繰り返そうとすると混乱をまねきやすいため、「文字」に対して処理をまわす場合はリスト（list of strings）持ちにしたほうがわかりやすいです（個人の感想）。

また、基本的にbaseの文字列処理よりもstringrを優先して使うようにしています（R>=4.1.0が必要だが、baseの文字列処理をstringiをラップした同名の関数でマスクできる[stringx](https://stringx.gagolewski.com/)というパッケージがあって、これを使うとタイプする文字数やヘルプを確認する頻度を減らせる）。

```{r intro-shorthands}
## ncharとpaste0だけマスクしておく
nchar <- stringr::str_length
paste0 <- stringr::str_c
```

### 00. 文字列の逆順

```{r intro-00}
stringr::str_split("stressed", pattern = "") %>%
  purrr::map(~ rev(.)) %>%
  unlist() %>%
  paste0(collapse = "")
```

### 01. 「パタトクカシーー」

```{r intro-01}
stringr::str_split("パタトクカシーー", pattern = "") %>%
  purrr::map(~ purrr::pluck(.[c(TRUE, FALSE)])) %>%
  unlist() %>%
  paste0(collapse = "")
```

### 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」

```{r intro-02}
list("パトカー", "タクシー") %>%
  purrr::map(~ stringr::str_split(., pattern = "")) %>%
  purrr::flatten() %>%
  purrr::pmap(~ paste0(.x, .y, collapse = "")) %>%
  unlist() %>%
  paste0(collapse = "")
```

### 03. 円周率

```{r intro-03}
stringr::str_split("Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.", pattern = " ") %>%
  purrr::flatten() %>%
  purrr::map(~ stringr::str_count(., pattern = "[:alpha:]")) %>%
  unlist()
```

### 04. 元素記号

```{r intro-04}
stringr::str_split("Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.", pattern = " ") %>%
  purrr::flatten() %>%
  purrr::imap(~
  dplyr::if_else(
    .y %in% c(1, 5, 6, 7, 8, 9, 15, 16, 19),
    stringr::str_sub(.x, 1, 1),
    stringr::str_sub(.x, 1, 2)
  )) %>%
  purrr::imap(function(x, i) {
    names(x) <- i
    return(x)
  }) %>%
  unlist()
```

### 05. n-gram

```{r intro-05}
ngram <- function(x, n = 2, sep = " ") {
  stopifnot(is.character(x))
  ## 先例がみんな`embed`を使っているが、ここでは使わない

  tokens <- unlist(stringr::str_split(x, pattern = sep))
  len <- length(tokens)

  if (len < n) {
    res <- character(0)
  } else {
    res <- sapply(1:max(1, len - n + 1), function(i) {
      paste0(tokens[i:min(len, i + n - 1)], collapse = " ")
    })
  }

  return(res)
}
ngram("I am an NLPer")
```

### 06. 集合

回答略

### 07. テンプレートによる文生成

回答略

### 08. 暗号文

```{r intro-08}
cipher <- function(str) {
  f <- purrr::as_mapper(~ 219 - .)
  v <- stringr::str_split(str, pattern = "", simplify = TRUE)
  res <- sapply(v[1, ], function(char) {
    dplyr::if_else(
      stringr::str_detect(char, "[:lower:]"),
      char %>%
        charToRaw() %>%
        as.integer() %>%
        f() %>%
        as.raw() %>%
        rawToChar(),
      char
    )
  })
  return(paste0(res, collapse = ""))
}
cipher("I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind.")
```

### 09. Typoglycemia

```{r intro-09}
typoglycemia <- function(str) {
  f <- function(char) {
    subset <- stringr::str_sub(char, 2, nchar(char) - 1) %>%
      stringr::str_split(pattern = "") %>%
      purrr::flatten() %>%
      sample()
    res <- paste0(
      c(
        stringr::str_sub(char, 1, 1),
        subset,
        stringr::str_sub(char, nchar(char), nchar(char))
      ),
      collapse = ""
    )
    return(res)
  }
  res <- stringr::str_split(str, pattern = " ") %>%
    purrr::flatten() %>%
    purrr::map(~
    dplyr::if_else(
      nchar(stringr::str_subset(., "[:alpha:]|:")) <= 4,
      .,
      f(.)
    ))
  return(paste0(res, collapse = " "))
}
typoglycemia("I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind.")
```

## UNIXコマンド

確認はやりません。~~だってWindowsだもん~~

### 10~15

素のテキストとして読んでもしょうがないので、以下のようなこと雰囲気でやります。

- 10. 行数のカウント
- 11. タブをスペースに置換
- 14. 先頭からN行を出力
- 15. 末尾のN行を出力

以下の２つはやりませんが、たぶん`fread(temp, select = c(1, 2))`みたいな感じで取れます。

- 12. 1列目をcol1.txtに，2列目をcol2.txtに保存
- 13. col1.txtとcol2.txtをマージ

```{r unixcmd-01}
txt <-
  data.table::fread(
    "https://nlp100.github.io/data/popular-names.txt",
    sep = "\t",
    quote = "",
    header = FALSE,
    col.names = c("name", "sex", "num_of_people", "year"),
    colClasses = list("character" = 1, "character" = 2, "integer" = 3, "integer" = 4),
    data.table = FALSE
  )

nrow(txt)
```

```{r unixcmd-02}
head(txt, 3)
```

```{r unixcmd-03}
tail(txt, 3)
```

### 16. ファイルをN分割する

```{r unixcmd-16}
split(txt, sort(rank(row.names(txt)) %% 5)) %>%
  purrr::map(~ head(.)) %>%
  print()
```

### 17. １列目の文字列の異なり

省略

### 18. 各行を3コラム目の数値の降順にソート

```{r unixcmd-18}
txt %>%
  dplyr::arrange(desc(num_of_people)) %>%
  head()
```

### 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる

```{r unixcmd-19}
purrr::map_dfr(txt$name, function(name) {
  stringr::str_split(name, pattern = "", simplify = TRUE) %>%
    t() %>%
    as.data.frame(stringsAsFactors = FALSE)
}) %>%
  dplyr::rename(string = V1) %>%
  dplyr::group_by(string) %>%
  dplyr::count(string, sort = TRUE) %>%
  dplyr::ungroup() %>%
  head()
```

## 正規表現

自然言語処理とはいったい

### 20. JSONデータの読み込み

```{r reprex-20}
jsonfile <- (function() {
  ## ファイルコネクションの閉じ忘れ防止に「閉包」に閉じ込めている
  ## ただし、実は.gz, .bz2, .xz, .zipファイルはリモートのファイルであってもread_linesで直接読むことが可能なので、
  ## 本当はこんなことする必要はない
  temp <- tempfile(fileext = ".gz")
  download.file("https://nlp100.github.io/data/jawiki-country.json.gz", temp, quiet = TRUE)
  con <- gzfile(description = temp, open = "rb")
  on.exit(close(con))
  readr::read_lines(con, locale = readr::locale(encoding = "UTF-8")) %>%
  purrr::map_dfr(~
    jsonlite::fromJSON(.))
})()

jsonfile %>%
  dplyr::filter(title == "イギリス") %>%
  dplyr::pull(text) %>%
  dplyr::glimpse() # 長いので
```

### 21. カテゴリ名を含む行を抽出

```{r reprex-21}
lines <- jsonfile %>%
  dplyr::filter(title == "イギリス") %>%
  dplyr::pull(text) %>%
  readr::read_lines() %>%
  stringr::str_subset(stringr::fixed("[[Category:"))
head(lines)
```

以下、回答略

## セッション情報

```{r sessioninfo}
sessioninfo::session_info()
```

