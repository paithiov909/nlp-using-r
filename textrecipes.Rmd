---
title: "Rによる自然言語処理（textrecipes, neologd, GloVe, XGBoost）"
author: "paithiov909"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)
```

## この記事について

[以前に書いた記事](https://github.com/paithiov909/wabbitspunch/blob/master/content/articles/about.md)を焼き直ししつつ、ばんくしさんの以下のブログ記事のまねをRでやってみます。

- [Rustによるlindera、neologd、fasttext、XGBoostを用いたテキスト分類 - Stimulator](https://vaaaaaanquish.hatenablog.com/entry/2020/12/14/192246)

ばんくしさんの記事は「Pythonどこまで脱却できるのか見るのも兼ねて」ということで、Rustで自然言語処理を試しています。私はべつに自然言語処理を実務でやるエンジニアとかではないですが、PythonじゃなくてRustとかGoといった静的型付けで速い言語で安全に書けたらうれしい場面があるよね、みたいなモチベーションなのかなと想像しています。

実際のところ、自分でコードを書きながら自然言語処理の真似事をするなら依然としてPythonが便利です。Rと比べても、Pythonには[SudachiPy](https://github.com/WorksApplications/SudachiPy)や[janome](https://mocobeta.github.io/janome/)といった選択肢がある一方で、RにはRコンソールからのみで導入が完了する形態素解析の手段が（少なくともCRANには）ありません。自然言語処理をやる言語としてPythonのほうがメジャーなことにはほかにもいくつかの理由というか経緯があるのでしょうが、Pythonを採用したほうがよいひとつのモチベーションとしては、テキストマイニングして得た特徴量を投入してディープラーニングをしたい場合は事実上Pythonを選択するしかないというのもある気がします。一応、[{keras}](https://keras.rstudio.com/)や[{torch}](https://github.com/mlverse/torch)というのもありますが、このあたりのパッケージを使うのはまだ趣味の領域な気がします。

そうはいっても、[{RMeCab}](https://sites.google.com/site/rmecab/)は強力なツールです。なぜかRからテキストマイニングに入ってしまった人間にとって、比較的簡単に導入できてほとんど環境を問わずすぐ使えるRMeCabは欠かせないツールだったことでしょう。ただ、Rに慣れてきていろいろなことをやってみたくなると、RMeCabは「なんか使いにくいな」みたいになりがちです。Rでも自然言語処理をやるためのパッケージは以下に紹介されているようにたくさんあるのですが、そもそも日本語情報があまりないし、そこそこがんばらないと詰まります。

https://twitter.com/dataandme/status/1092509662384189441

でもまあRでもできなくはないんだよというのをしめす目的で、ここでは先の記事と同じようなことをRでやっていきます。

## パッケージの選定にあたって

### Tokenization

形態素解析などをやるパッケージとしては次に挙げるようなものがあります。知るかぎりではぜんぶ個人開発で、環境や使用する辞書、解析する文字列などによって上手く動いたり動かなかったりします。Neologd辞書を使うならRcppMeCabにすべきですが、メンテナの人が最近忙しいとかで、あまりアクティブに開発されていません。

- [IshidaMotohiro/RMeCab: Interface to MeCab](https://github.com/IshidaMotohiro/RMeCab)
- [junhewk/RcppMeCab: RcppMeCab: Rcpp Interface of CJK Morpheme Analyzer MeCab](https://github.com/junhewk/RcppMeCab)
- [uribo/sudachir: R Interface to 'Sudachi'](https://github.com/uribo/sudachir)

Universal Dependenciesなら次が使えます。udpipeはC++実装のラッパー、spacyrはPythonバックエンドです。

- [bnosac/udpipe: R package for Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing Based on the UDPipe Natural Language Processing Toolkit](https://github.com/bnosac/udpipe)
- [quanteda/spacyr: R wrapper to spaCy NLP](https://github.com/quanteda/spacyr)

最近になってbnosacからBPE（Byte Pair Encoding）とsentencepieceのRラッパーがリリースされました。

- [bnosac/tokenizers.bpe: R package for Byte Pair Encoding based on YouTokenToMe](https://github.com/bnosac/tokenizers.bpe)
- [bnosac/sentencepiece: R package for Byte Pair Encoding / Unigram modelling based on Sentencepiece](https://github.com/bnosac/sentencepiece)

### Language Representations

アクティブに開発されていそうなものとして、このあたりのパッケージがあります。

- [dselivanov/text2vec: Fast vectorization, topic modeling, distances and GloVe word embeddings in R.](https://github.com/dselivanov/text2vec)
- [bnosac/ruimtehol: R package to Embed All the Things! using StarSpace](https://github.com/bnosac/ruimtehol)
- [bnosac/doc2vec: Distributed Representations of Sentences and Documents](https://github.com/bnosac/doc2vec)
- [bnosac/golgotha: Contextualised Embeddings and Language Modelling using BERT and Friends using R](https://github.com/bnosac/golgotha)
- [jonathanbratt/RBERT: Implementation of BERT in R](https://github.com/jonathanbratt/RBERT)

## セットアップ

```{r load_libs}
require(tidymodels)
options(mecabSysDic = file.path(system("mecab-config --dicdir", intern = TRUE), "mecab-ipadic-neologd"))

tidymodels::tidymodels_prefer()
```

## データの準備

[livedoorニュースコーパス](https://www.rondhuit.com/download.html#ldcc)を使います。以下の9カテゴリです。

- トピックニュース
- Sports Watch
- ITライフハック
- 家電チャンネル
- MOVIE ENTER
- 独女通信
- エスマックス
- livedoor HOMME
- Peachy

[パーサを書いた](https://github.com/paithiov909/ldccr)ので、それでデータフレームにします。

```{r prep_corpus_1}
corpus <- ldccr::read_ldnws()
```

```{r prep_corpus_2}
corpus <- corpus %>%
  dplyr::select(-file_path) %>%
  dplyr::mutate(category = as.factor(category)) %>%
  dplyr::mutate(body = audubon::strj_normalize(body)) %>%
  tibble::rowid_to_column()
```

この記事を書いた当初はRcppMeCab + Neologdでの形態素解析を試そうとして上手くいかず、[rjavacmecab](https://github.com/paithiov909/rjavacmecab)という自作パッケージで代用していました（動くけど非常に遅い）。その後、RcppMeCabを使う場合には次のような感じでできるのを確認しました。ただし、2021年1月現在CRANにあるバージョン（0.0.1.2）は未知語の処理にバグがあるようなので、ここでは[ソースを修正したもの](https://github.com/paithiov909/RcppMeCab)を使っています。

```{r prep_corpus_3}
corpus <- corpus %>%
  dplyr::pull(body) %>%
  RcppMeCab::posParallel(format = "data.frame") %>%
  tidyr::drop_na() %>%
  audubon::pack() %>%
  dplyr::mutate(doc_id = as.integer(doc_id)) %>%
  dplyr::left_join(corpus, by = c("doc_id" = "rowid"))
```

こういうデータになります。

```{r glimpse_corpus}
corpus <- corpus %>%
  dplyr::select(doc_id, category, text) %>%
  dplyr::glimpse()
```

## モデルの学習１（FeatureHashing）

データを分割します。

```{r prep_data}
dat <- rsample::initial_split(corpus, prop = .8)
train <- rsample::training(dat)
test <- rsample::testing(dat)
```

以下のレシピとモデルで学習します。ハッシュトリックを使っています。デフォルトだとパラメータは[ここに書いている感じ](https://parsnip.tidymodels.org/reference/boost_tree.html)になります。

```{r prep_model_1}
model <-
  parsnip::boost_tree(
    sample_size = tune::tune(),
    loss_reduction = tune::tune(),
    tree_depth = tune::tune()
  ) %>%
  parsnip::set_engine("xgboost") %>%
  parsnip::set_mode("classification")

space_tokenizer <- function(x) {
  strsplit(x, " +")
}

rec_fh <-
  recipes::recipe(
    category ~ text,
    data = train
  ) %>%
    textrecipes::step_tokenize(text, custom_token = space_tokenizer) %>%
    textrecipes::step_tokenfilter(text, min_times = 10L, max_tokens = 200L) %>%
    textrecipes::step_tfidf(text) %>%
    embed::step_pca_sparse(recipes::all_numeric(), num_comp = 10L, predictor_prop = 2/3)
```



```{r workflow_1}
wl <-
  workflows::workflow() %>%
  workflows::add_model(model) %>%
  workflows::add_recipe(rec_fh)
```

```{r tune_wl}
doParallel::registerDoParallel(cores = parallel::detectCores())

res <-
  wl %>%
  tune::tune_grid(
    resamples = rsample::vfold_cv(train, v = 5L),
    grid = dials::grid_latin_hypercube(
      dials::sample_prop(),
      dials::loss_reduction(),
      dials::tree_depth(),
      size = 5L
    ),
    metrics = yardstick::metric_set(yardstick::accuracy),
    control = tune::control_grid(save_pred = TRUE)
  )

doParallel::stopImplicitCluster()
```

```{r autoplot}
ggplot2::autoplot(res)
```


```{r fit_tfidf}
res_tfidf <-
  tune::finalize_workflow(wl, tune::select_best(res, metric = "accuracy"))

res_tfidf <- parsnip::fit(res_tfidf, train)
```

```{r pred_tfidf}
pred <-
  dplyr::bind_cols(dplyr::select(test, category), predict(res_tfidf, test))

yardstick::accuracy(pred, truth = category, estimate = .pred_class)
```


## モデルの学習２（fasttext）

BPEでサブワード分割した文書について、fasttextで埋め込みを得てから学習してみます。あくまで試してみる目的のため、ここでは50次元のベクトルにします。

```{r make_ft_model}
temp <- tempfile(fileext = ".txt")
readr::write_lines(
  corpus_bpe$text,
  file = temp
)
fastrtext::execute(c("skipgram", "-input", temp, "-output", "model", "-dim", 50L))
```

モデルの読み込み。

```{r load_ft_model}
ft_model <- fastrtext::load_model("model.bin")
```

データフレームに整形します。

```{r reshape_ft_model}
vocab <- fastrtext::get_dictionary(ft_model)
Encoding(vocab) <- "UTF-8" ## For Windows machine
embeddings <- ft_model %>%
  fastrtext::get_word_vectors(vocab) %>%
  as.data.frame() %>%
  dplyr::bind_cols(tibble::tibble(vocab = vocab)) %>%
  dplyr::select(vocab, everything())

remove(ft_model)
```

今度は次のようなレシピで学習します。上でやったのと同じように素直にやろうとすると手元の環境ではメモリが足りなかったので、ここでは`textrecipes::step_tokenfilter`で語彙を削っています。

```{r prep_model_2}
fit_corpus_b <- function(corpus, model) {
  recipe <- recipes::recipe(
    category ~ text,
    data = rsample::training(corpus)
  ) %>%
    textrecipes::step_tokenize(text) %>%
    textrecipes::step_tokenfilter(text, min_times = 3L, max_tokens = 50L) %>%
    textrecipes::step_word_embeddings(
      text,
      embeddings = tibble::as_tibble(embeddings)
    ) %>%
    recipes::prep()

  split <- prep_corpus(corpus, recipe)

  fitted <- model %>%
    parsnip::fit(category ~ ., data = split$train)

  fitted %>%
    predict(split$test) %>%
    dplyr::bind_cols(split$test) %>%
    yardstick::metrics(truth = category, estimate = .pred_class) %>%
    print()
}
```

`fit`します（なんか思ったほど精度がよくないのでどこか間違っているのかもしれない）。

```{r cleanup, include=FALSE}
remove(corpus_bpe)
remove(corpus_mecab)
remove(data)
remove(token_model)
gc()
gc()
```

```{r fit_and_predict_2}
fit_corpus_b(corpus$bpe, model)
```

## 所感

livedoorニュースコーパスはそんなに大きなコーパスじゃないのでサクッとできてほしいのですが、textrecipesで埋め込みを扱うには手元の環境では不足なようでした（たぶん私が使っているPCが古いだけですが）。

このコーパスのカテゴリ分類はかなり易しいタスクであることが知られている（というか、一部のカテゴリではそのカテゴリを同定できる単語が本文に含まれてしまっている）ので相性もあるのでしょうが、ハッシュトリックしてXGBoostに投入するだけで簡単によい精度の予測ができる点は気持ちよいです。bnosac/sentencepieceもtokenizer.bpeと同じノリで使えるようなので、サブワード分割でよければRでも簡単にできるとわかりました。

一方で、RcppMeCabは使えるようにするまでが依然としてめんどくさいということもあり、分かち書きするだけならほかの手段のほうが簡単な気がします。

## セッション情報

```{r session_info}
sessioninfo::session_info()
```
